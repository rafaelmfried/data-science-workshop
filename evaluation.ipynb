{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8e2b60c-8792-4c80-9a5d-9a63293cb1e3",
   "metadata": {},
   "source": [
    "# Avaliação de Modelos de Classificação\n",
    "\n",
    "Para avaliar o desempenho de um modelo de classificação, é fundamental utilizar diferentes métricas, pois cada uma oferece uma perspectiva distinta sobre como o modelo está se comportando. A seguir, detalho as principais métricas de avaliação:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b61fc6-3b3b-42c6-97f2-6f84b27c80f4",
   "metadata": {},
   "source": [
    "## 1. Acurácia (Accuracy)\n",
    "\n",
    "**Definição:** Proporção de predições corretas em relação ao total de predições realizadas.\n",
    "\n",
    "**Fórmula:**\n",
    "\n",
    "\\[ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} \\]\n",
    "\n",
    "- **Vantagem:** Fácil de interpretar.\n",
    "- **Limitação:** Pode ser enganosa em datasets desbalanceados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d17b8d1-0a03-4c6d-9ea3-20b1c3d3bb5d",
   "metadata": {},
   "source": [
    "## 2. Matriz de Confusão\n",
    "\n",
    "A matriz de confusão apresenta a distribuição de acertos e erros do modelo, dividindo as predições em:\n",
    "\n",
    "- **Verdadeiros Positivos (TP):** Casos em que o modelo previu positivo e o rótulo real é positivo.\n",
    "- **Verdadeiros Negativos (TN):** Casos em que o modelo previu negativo e o rótulo real é negativo.\n",
    "- **Falsos Positivos (FP):** Casos em que o modelo previu positivo, mas o rótulo real é negativo.\n",
    "- **Falsos Negativos (FN):** Casos em que o modelo previu negativo, mas o rótulo real é positivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b3f73e-9e97-4c60-9f5c-2ac0de9f4e8a",
   "metadata": {},
   "source": [
    "## 3. Precisão (Precision) e Revocação (Recall)\n",
    "\n",
    "**Precisão:**\n",
    "\n",
    "- **Definição:** Proporção de predições positivas que são corretas.\n",
    "\n",
    "  \\[ \\text{Precision} = \\frac{TP}{TP + FP} \\]\n",
    "\n",
    "**Revocação (Recall) ou Sensibilidade:**\n",
    "\n",
    "- **Definição:** Proporção de exemplos positivos corretamente identificados.\n",
    "\n",
    "  \\[ \\text{Recall} = \\frac{TP}{TP + FN} \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb3a2e8-1783-4e70-b033-462174a64d57",
   "metadata": {},
   "source": [
    "## 4. F1-Score\n",
    "\n",
    "**Definição:** Média harmônica entre precisão e revocação.\n",
    "\n",
    "**Fórmula:**\n",
    "\n",
    "  \\[ F1 = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\]\n",
    "\n",
    "- **Interpretação:** Um F1-Score alto indica um bom equilíbrio entre precisão e revocação, sendo especialmente útil em cenários com classes desbalanceadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d5d2c2-1e89-4e1d-ae57-6cf9d6703746",
   "metadata": {},
   "source": [
    "## 5. Curva ROC e AUC\n",
    "\n",
    "**Curva ROC (Receiver Operating Characteristic):**\n",
    "\n",
    "- **Definição:** Gráfico que relaciona a taxa de verdadeiros positivos (Recall) e a taxa de falsos positivos para diferentes limiares de classificação.\n",
    "\n",
    "**AUC (Area Under the ROC Curve):**\n",
    "\n",
    "- **Definição:** A área sob a curva ROC. Varia de 0 a 1, onde valores próximos a 1 indicam um excelente desempenho do modelo.\n",
    "\n",
    "- **Utilidade:** Fornece uma medida agregada do desempenho do modelo em todos os possíveis limiares de decisão."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c9db44-f5ef-4d30-87a6-d86d7f12b2e2",
   "metadata": {},
   "source": [
    "## 6. Exemplo Prático\n",
    "\n",
    "A seguir, um exemplo prático utilizando dados fictícios para demonstrar o cálculo dessas métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4246733-9b77-4e6a-b40f-44f6ceeb9bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Exemplo fictício de rótulos verdadeiros e predições\n",
    "y_true = np.array([0, 1, 0, 1, 0, 1, 1, 0, 0, 1])\n",
    "y_pred = np.array([0, 1, 0, 0, 0, 1, 1, 0, 1, 1])\n",
    "\n",
    "# Cálculo de Acurácia\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(\"Acurácia:\", acc)\n",
    "\n",
    "# Matriz de Confusão\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Matriz de Confusão:\\n\", cm)\n",
    "\n",
    "# Relatório de Classificação (incluindo precisão, recall e F1-score)\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(\"Relatório de Classificação:\\n\", report)\n",
    "\n",
    "# Exemplo de probabilidades preditas para a classe positiva\n",
    "y_probs = np.array([0.1, 0.9, 0.2, 0.4, 0.3, 0.8, 0.7, 0.2, 0.6, 0.85])\n",
    "\n",
    "# Cálculo da curva ROC\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, label=f'Curva ROC (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Taxa de Falsos Positivos')\n",
    "plt.ylabel('Taxa de Verdadeiros Positivos')\n",
    "plt.title('Curva ROC')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468e8d2d-3ac4-43f9-a8e8-9da7c82d0c15",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "Cada métrica de avaliação oferece uma perspectiva diferente do desempenho do modelo:\n",
    "- **Acurácia:** Útil quando as classes estão equilibradas.\n",
    "- **Matriz de Confusão:** Permite identificar os tipos de erros (falsos positivos e falsos negativos).\n",
    "- **Precisão e Recall:** Essenciais quando há custo diferente para os tipos de erro.\n",
    "- **F1-Score:** Um bom indicador global em casos de classes desbalanceadas.\n",
    "- **Curva ROC e AUC:** Avaliam o desempenho do modelo em diferentes limiares de decisão.\n",
    "\n",
    "Compreender essas métricas é fundamental para interpretar corretamente os resultados e ajustar o modelo conforme as necessidades do problema."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
